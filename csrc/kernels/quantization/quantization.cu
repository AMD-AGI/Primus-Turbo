// Copyright (c) 2025, Advanced Micro Devices, Inc. All rights reserved.
//
// See LICENSE for license information.

#include "primus_turbo/common.h"
#include "primus_turbo/elementwise/unary_kernel_template.cuh"
#include "primus_turbo/memory_pack.h"
#include "primus_turbo/quantization.h"

namespace primus_turbo {

using namespace primus_turbo::dtype;

template <typename ComputeType = float> struct QuantOpBase {
    static PRIMUS_TURBO_HOST_DEVICE ComputeType quant(const ComputeType x, const ComputeType scale,
                                                      const ComputeType clip_min,
                                                      const ComputeType clip_max) {
        const ComputeType v = x * scale;
        return fmax(fmin(v, clip_max), clip_min);
    }
};

template <typename ComputeType = float>
struct QuantTensorwiseScalePtrOp : QuantOpBase<ComputeType> {
    const ComputeType *scale_ptr;
    ComputeType        clip_min;
    ComputeType        clip_max;

    PRIMUS_TURBO_HOST_DEVICE ComputeType operator()(ComputeType x) const {
        const ComputeType scale = scale_ptr[0];
        return QuantOpBase<ComputeType>::quant(x, scale, clip_min, clip_max);
    }
};

template <typename ComputeType = float> struct DeQuantTensorwiseScaleInvPtrOp {
    const ComputeType *scale_inv_ptr;

    PRIMUS_TURBO_HOST_DEVICE ComputeType operator()(ComputeType x) const {
        const ComputeType scale_inv = scale_inv_ptr[0];
        return x * scale_inv;
    }
};

template <typename T>
__global__ void compute_scale_from_amax_kernel(const T *amax_ptr, const T q_max, T *scale_ptr,
                                               T *scale_inv_ptr, const int64_t n, const float eps) {
    int64_t tid = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;
    if (tid < n) {
        float amax         = static_cast<float>(amax_ptr[tid]);
        amax               = fmax(amax, eps);
        float scale        = static_cast<float>(q_max) / amax;
        float scale_inv    = 1.0f / scale;
        scale_ptr[tid]     = static_cast<T>(scale);
        scale_inv_ptr[tid] = static_cast<T>(scale_inv);
    }
}

template <typename T>
void compute_scale_from_amax(const T *amax, const T q_max, T *scale, T *scale_inv, const int64_t n,
                             hipStream_t stream, const float eps) {
    const int64_t BLOCK_SIZE = 512;
    const int64_t GRID_SIZE  = DIVUP<int64_t>(n, BLOCK_SIZE);
    compute_scale_from_amax_kernel<T>
        <<<GRID_SIZE, BLOCK_SIZE, 0, stream>>>(amax, q_max, scale, scale_inv, n, eps);
}

template <typename FType, typename QType, typename ComputeType>
void quantize_tensorwise_impl(const FType *x, const float *scale, QType *y, const int64_t n,
                              hipStream_t stream) {
    QuantTensorwiseScalePtrOp<ComputeType> op{
        {},
        reinterpret_cast<const ComputeType *>(scale),
        static_cast<ComputeType>(std::numeric_limits<QType>::lowest()),
        static_cast<ComputeType>(std::numeric_limits<QType>::max())};

    const int32_t BLOCK_SIZE = 512;

    int32_t pack_size = std::min(get_pack_size<FType>(x), get_pack_size<QType>(y));
    switch (pack_size) {
    case 8: {
        const int32_t       UNROLL = valid_pack<FType, 8>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, FType, QType, QuantTensorwiseScalePtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 4: {
        const int32_t       UNROLL = valid_pack<FType, 4>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, FType, QType, QuantTensorwiseScalePtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 2: {
        const int32_t       UNROLL = valid_pack<FType, 2>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, FType, QType, QuantTensorwiseScalePtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 1: {
        PackedEltwiseConfig pack_cfg(n, 1, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, 1, FType, QType, QuantTensorwiseScalePtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    default:
        PRIMUS_TURBO_ERROR("Error Pack Size");
        break;
    }
}

template <typename FType, typename QType, typename ComputeType>
void dequantize_tensorwise_impl(const QType *x, const float *scale_inv, FType *y, const int64_t n,
                                hipStream_t stream) {
    DeQuantTensorwiseScaleInvPtrOp<ComputeType> op{
        reinterpret_cast<const ComputeType *>(scale_inv),
    };

    const int32_t BLOCK_SIZE = 512;
    int32_t       pack_size  = std::min(get_pack_size<QType>(x), get_pack_size<FType>(y));
    switch (pack_size) {
    case 8: {
        const int32_t       UNROLL = valid_pack<FType, 8>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, QType, FType, DeQuantTensorwiseScaleInvPtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 4: {
        const int32_t       UNROLL = valid_pack<FType, 4>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, QType, FType, DeQuantTensorwiseScaleInvPtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 2: {
        const int32_t       UNROLL = valid_pack<FType, 2>();
        PackedEltwiseConfig pack_cfg(n, UNROLL, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, UNROLL, QType, FType, DeQuantTensorwiseScaleInvPtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    case 1: {
        PackedEltwiseConfig pack_cfg(n, 1, BLOCK_SIZE);
        unary_kernel<BLOCK_SIZE, 1, QType, FType, DeQuantTensorwiseScaleInvPtrOp<ComputeType>>
            <<<pack_cfg.nBlock, BLOCK_SIZE, 0, stream>>>(x, y, op, pack_cfg);
        break;
    }
    default:
        PRIMUS_TURBO_ERROR("Error Pack Size");
        break;
    }
}

// ****  ****
template void compute_scale_from_amax<float>(const float *amax, float q_max, float *scale,
                                             float *scale_inv, const int64_t n, hipStream_t stream,
                                             const float eps);

#define DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(FType, QType)                                   \
    template void quantize_tensorwise_impl<FType, QType>(                                          \
        const FType *x, const float *scale, QType *y, const int64_t n, hipStream_t stream);        \
    template void dequantize_tensorwise_impl<FType, QType>(                                        \
        const QType *x, const float *scale_inv, FType *y, const int64_t n, hipStream_t stream);

DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::float16, dtype::float8_e4m3)
DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::float16, dtype::float8_e5m2)
DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::bfloat16, dtype::float8_e4m3)
DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::bfloat16, dtype::float8_e5m2)
DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::float32, dtype::float8_e4m3)
DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE(dtype::float32, dtype::float8_e5m2)

#undef DECL_QUANT_ADN_DEQUANT_TENSORWISE_INSTANCE

} // namespace primus_turbo
