###############################################################################
# Copyright (c) 2022-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
# Modification CopyrightÂ© 2025 Advanced Micro Devices, Inc. All rights reserved.
#
# See LICENSE for license information.

###############################################################################

import functools
import math

import torch


def get_random_sign_vector(device: int) -> torch.Tensor:
    """Hard-coded random signs for Hadamard transform.

    https://xkcd.com/221/

    """

    # fmt: off
    return torch.tensor(
        [1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1, 1, 1, 1, -1, 1, -1, -1, -1, -1, -1, -1, 1, -1, 1, -1, -1],
        dtype=torch.float32,
        device=device,
    )
    # fmt: on


def get_hadamard_matrix(hadamard_dimension: int, device: int) -> torch.Tensor:
    """Construct a 32x32 Hadamard matrix."""
    assert hadamard_dimension == 32, "Only hadamard dimension 32 is supported."
    hadamard_scale = 1 / math.sqrt(hadamard_dimension)
    # fmt: off
    return (
        torch.tensor(
            [[ 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],
            [ 1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1],
            [ 1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1],
            [ 1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1],
            [ 1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1],
            [ 1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1],
            [ 1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1],
            [ 1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1],
            [ 1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1],
            [ 1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1],
            [ 1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1],
            [ 1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1],
            [ 1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1],
            [ 1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1],
            [ 1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1],
            [ 1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1],
            [ 1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1],
            [ 1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1],
            [ 1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1],
            [ 1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1],
            [ 1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1],
            [ 1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1],
            [ 1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1, -1],
            [ 1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1, -1],
            [ 1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1, -1],
            [ 1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1,  1],
            [ 1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1, -1],
            [ 1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1, -1],
            [ 1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1,  1],
            [ 1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1, -1],
            [ 1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1, -1],
            [ 1, -1, -1,  1, -1, -1,  1, -1, -1, -1, -1,  1,  1,  1, -1,  1, -1,  1, -1, -1, -1,  1,  1,  1,  1, -1,  1,  1, -1,  1,  1,  1]],
            dtype=torch.float32,
            device=device,
        )
        * hadamard_scale
    )
    # fmt: on


@functools.lru_cache(maxsize=None)
def get_rht_matrix(dtype: torch.dtype, device: int) -> torch.Tensor:
    """Construct matrix used in random Hadamard transform."""
    hadamard_dimension = 32
    signs = get_random_sign_vector(device=device)
    sign_matrix = signs * torch.eye(hadamard_dimension, dtype=torch.float32, device=device)
    rht_matrix = sign_matrix @ get_hadamard_matrix(hadamard_dimension, device=device)

    return rht_matrix.to(dtype=dtype)


def apply_random_hadamard_transform(x: torch.Tensor, block_size: int) -> torch.Tensor:
    # RHT dimension equals the quantization tile length
    rht_dim = block_size
    assert (
        x.shape[-1] % rht_dim == 0
    ), f"Inner dimension {x.shape[-1]} must be divisible by hadamard dimension {rht_dim}"

    # Build H and scale
    H = get_rht_matrix(x.dtype, x.device)

    # Perform blockwise transform along the last dimension
    original_shape = x.shape
    x_mat = x.contiguous().view(-1, rht_dim)
    # Random sign matrix is identity in this reference (no sign flipping)
    out = x_mat @ H

    return out.view(original_shape)
